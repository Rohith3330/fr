{"ast":null,"code":"import _slicedToArray from \"C:\\\\Users\\\\Rohith\\\\mern\\\\frontend\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/helpers/esm/slicedToArray\";\nimport _asyncToGenerator from \"C:\\\\Users\\\\Rohith\\\\mern\\\\frontend\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/helpers/esm/asyncToGenerator\";\nimport _classCallCheck from \"C:\\\\Users\\\\Rohith\\\\mern\\\\frontend\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/helpers/esm/classCallCheck\";\nimport _createClass from \"C:\\\\Users\\\\Rohith\\\\mern\\\\frontend\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/helpers/esm/createClass\";\nfunction _regeneratorRuntime() { \"use strict\"; /*! regenerator-runtime -- Copyright (c) 2014-present, Facebook, Inc. -- license (MIT): https://github.com/facebook/regenerator/blob/main/LICENSE */ _regeneratorRuntime = function _regeneratorRuntime() { return exports; }; var exports = {}, Op = Object.prototype, hasOwn = Op.hasOwnProperty, $Symbol = \"function\" == typeof Symbol ? Symbol : {}, iteratorSymbol = $Symbol.iterator || \"@@iterator\", asyncIteratorSymbol = $Symbol.asyncIterator || \"@@asyncIterator\", toStringTagSymbol = $Symbol.toStringTag || \"@@toStringTag\"; function define(obj, key, value) { return Object.defineProperty(obj, key, { value: value, enumerable: !0, configurable: !0, writable: !0 }), obj[key]; } try { define({}, \"\"); } catch (err) { define = function define(obj, key, value) { return obj[key] = value; }; } function wrap(innerFn, outerFn, self, tryLocsList) { var protoGenerator = outerFn && outerFn.prototype instanceof Generator ? outerFn : Generator, generator = Object.create(protoGenerator.prototype), context = new Context(tryLocsList || []); return generator._invoke = function (innerFn, self, context) { var state = \"suspendedStart\"; return function (method, arg) { if (\"executing\" === state) throw new Error(\"Generator is already running\"); if (\"completed\" === state) { if (\"throw\" === method) throw arg; return doneResult(); } for (context.method = method, context.arg = arg;;) { var delegate = context.delegate; if (delegate) { var delegateResult = maybeInvokeDelegate(delegate, context); if (delegateResult) { if (delegateResult === ContinueSentinel) continue; return delegateResult; } } if (\"next\" === context.method) context.sent = context._sent = context.arg;else if (\"throw\" === context.method) { if (\"suspendedStart\" === state) throw state = \"completed\", context.arg; context.dispatchException(context.arg); } else \"return\" === context.method && context.abrupt(\"return\", context.arg); state = \"executing\"; var record = tryCatch(innerFn, self, context); if (\"normal\" === record.type) { if (state = context.done ? \"completed\" : \"suspendedYield\", record.arg === ContinueSentinel) continue; return { value: record.arg, done: context.done }; } \"throw\" === record.type && (state = \"completed\", context.method = \"throw\", context.arg = record.arg); } }; }(innerFn, self, context), generator; } function tryCatch(fn, obj, arg) { try { return { type: \"normal\", arg: fn.call(obj, arg) }; } catch (err) { return { type: \"throw\", arg: err }; } } exports.wrap = wrap; var ContinueSentinel = {}; function Generator() {} function GeneratorFunction() {} function GeneratorFunctionPrototype() {} var IteratorPrototype = {}; define(IteratorPrototype, iteratorSymbol, function () { return this; }); var getProto = Object.getPrototypeOf, NativeIteratorPrototype = getProto && getProto(getProto(values([]))); NativeIteratorPrototype && NativeIteratorPrototype !== Op && hasOwn.call(NativeIteratorPrototype, iteratorSymbol) && (IteratorPrototype = NativeIteratorPrototype); var Gp = GeneratorFunctionPrototype.prototype = Generator.prototype = Object.create(IteratorPrototype); function defineIteratorMethods(prototype) { [\"next\", \"throw\", \"return\"].forEach(function (method) { define(prototype, method, function (arg) { return this._invoke(method, arg); }); }); } function AsyncIterator(generator, PromiseImpl) { function invoke(method, arg, resolve, reject) { var record = tryCatch(generator[method], generator, arg); if (\"throw\" !== record.type) { var result = record.arg, value = result.value; return value && \"object\" == typeof value && hasOwn.call(value, \"__await\") ? PromiseImpl.resolve(value.__await).then(function (value) { invoke(\"next\", value, resolve, reject); }, function (err) { invoke(\"throw\", err, resolve, reject); }) : PromiseImpl.resolve(value).then(function (unwrapped) { result.value = unwrapped, resolve(result); }, function (error) { return invoke(\"throw\", error, resolve, reject); }); } reject(record.arg); } var previousPromise; this._invoke = function (method, arg) { function callInvokeWithMethodAndArg() { return new PromiseImpl(function (resolve, reject) { invoke(method, arg, resolve, reject); }); } return previousPromise = previousPromise ? previousPromise.then(callInvokeWithMethodAndArg, callInvokeWithMethodAndArg) : callInvokeWithMethodAndArg(); }; } function maybeInvokeDelegate(delegate, context) { var method = delegate.iterator[context.method]; if (undefined === method) { if (context.delegate = null, \"throw\" === context.method) { if (delegate.iterator.return && (context.method = \"return\", context.arg = undefined, maybeInvokeDelegate(delegate, context), \"throw\" === context.method)) return ContinueSentinel; context.method = \"throw\", context.arg = new TypeError(\"The iterator does not provide a 'throw' method\"); } return ContinueSentinel; } var record = tryCatch(method, delegate.iterator, context.arg); if (\"throw\" === record.type) return context.method = \"throw\", context.arg = record.arg, context.delegate = null, ContinueSentinel; var info = record.arg; return info ? info.done ? (context[delegate.resultName] = info.value, context.next = delegate.nextLoc, \"return\" !== context.method && (context.method = \"next\", context.arg = undefined), context.delegate = null, ContinueSentinel) : info : (context.method = \"throw\", context.arg = new TypeError(\"iterator result is not an object\"), context.delegate = null, ContinueSentinel); } function pushTryEntry(locs) { var entry = { tryLoc: locs[0] }; 1 in locs && (entry.catchLoc = locs[1]), 2 in locs && (entry.finallyLoc = locs[2], entry.afterLoc = locs[3]), this.tryEntries.push(entry); } function resetTryEntry(entry) { var record = entry.completion || {}; record.type = \"normal\", delete record.arg, entry.completion = record; } function Context(tryLocsList) { this.tryEntries = [{ tryLoc: \"root\" }], tryLocsList.forEach(pushTryEntry, this), this.reset(!0); } function values(iterable) { if (iterable) { var iteratorMethod = iterable[iteratorSymbol]; if (iteratorMethod) return iteratorMethod.call(iterable); if (\"function\" == typeof iterable.next) return iterable; if (!isNaN(iterable.length)) { var i = -1, next = function next() { for (; ++i < iterable.length;) { if (hasOwn.call(iterable, i)) return next.value = iterable[i], next.done = !1, next; } return next.value = undefined, next.done = !0, next; }; return next.next = next; } } return { next: doneResult }; } function doneResult() { return { value: undefined, done: !0 }; } return GeneratorFunction.prototype = GeneratorFunctionPrototype, define(Gp, \"constructor\", GeneratorFunctionPrototype), define(GeneratorFunctionPrototype, \"constructor\", GeneratorFunction), GeneratorFunction.displayName = define(GeneratorFunctionPrototype, toStringTagSymbol, \"GeneratorFunction\"), exports.isGeneratorFunction = function (genFun) { var ctor = \"function\" == typeof genFun && genFun.constructor; return !!ctor && (ctor === GeneratorFunction || \"GeneratorFunction\" === (ctor.displayName || ctor.name)); }, exports.mark = function (genFun) { return Object.setPrototypeOf ? Object.setPrototypeOf(genFun, GeneratorFunctionPrototype) : (genFun.__proto__ = GeneratorFunctionPrototype, define(genFun, toStringTagSymbol, \"GeneratorFunction\")), genFun.prototype = Object.create(Gp), genFun; }, exports.awrap = function (arg) { return { __await: arg }; }, defineIteratorMethods(AsyncIterator.prototype), define(AsyncIterator.prototype, asyncIteratorSymbol, function () { return this; }), exports.AsyncIterator = AsyncIterator, exports.async = function (innerFn, outerFn, self, tryLocsList, PromiseImpl) { void 0 === PromiseImpl && (PromiseImpl = Promise); var iter = new AsyncIterator(wrap(innerFn, outerFn, self, tryLocsList), PromiseImpl); return exports.isGeneratorFunction(outerFn) ? iter : iter.next().then(function (result) { return result.done ? result.value : iter.next(); }); }, defineIteratorMethods(Gp), define(Gp, toStringTagSymbol, \"Generator\"), define(Gp, iteratorSymbol, function () { return this; }), define(Gp, \"toString\", function () { return \"[object Generator]\"; }), exports.keys = function (object) { var keys = []; for (var key in object) { keys.push(key); } return keys.reverse(), function next() { for (; keys.length;) { var key = keys.pop(); if (key in object) return next.value = key, next.done = !1, next; } return next.done = !0, next; }; }, exports.values = values, Context.prototype = { constructor: Context, reset: function reset(skipTempReset) { if (this.prev = 0, this.next = 0, this.sent = this._sent = undefined, this.done = !1, this.delegate = null, this.method = \"next\", this.arg = undefined, this.tryEntries.forEach(resetTryEntry), !skipTempReset) for (var name in this) { \"t\" === name.charAt(0) && hasOwn.call(this, name) && !isNaN(+name.slice(1)) && (this[name] = undefined); } }, stop: function stop() { this.done = !0; var rootRecord = this.tryEntries[0].completion; if (\"throw\" === rootRecord.type) throw rootRecord.arg; return this.rval; }, dispatchException: function dispatchException(exception) { if (this.done) throw exception; var context = this; function handle(loc, caught) { return record.type = \"throw\", record.arg = exception, context.next = loc, caught && (context.method = \"next\", context.arg = undefined), !!caught; } for (var i = this.tryEntries.length - 1; i >= 0; --i) { var entry = this.tryEntries[i], record = entry.completion; if (\"root\" === entry.tryLoc) return handle(\"end\"); if (entry.tryLoc <= this.prev) { var hasCatch = hasOwn.call(entry, \"catchLoc\"), hasFinally = hasOwn.call(entry, \"finallyLoc\"); if (hasCatch && hasFinally) { if (this.prev < entry.catchLoc) return handle(entry.catchLoc, !0); if (this.prev < entry.finallyLoc) return handle(entry.finallyLoc); } else if (hasCatch) { if (this.prev < entry.catchLoc) return handle(entry.catchLoc, !0); } else { if (!hasFinally) throw new Error(\"try statement without catch or finally\"); if (this.prev < entry.finallyLoc) return handle(entry.finallyLoc); } } } }, abrupt: function abrupt(type, arg) { for (var i = this.tryEntries.length - 1; i >= 0; --i) { var entry = this.tryEntries[i]; if (entry.tryLoc <= this.prev && hasOwn.call(entry, \"finallyLoc\") && this.prev < entry.finallyLoc) { var finallyEntry = entry; break; } } finallyEntry && (\"break\" === type || \"continue\" === type) && finallyEntry.tryLoc <= arg && arg <= finallyEntry.finallyLoc && (finallyEntry = null); var record = finallyEntry ? finallyEntry.completion : {}; return record.type = type, record.arg = arg, finallyEntry ? (this.method = \"next\", this.next = finallyEntry.finallyLoc, ContinueSentinel) : this.complete(record); }, complete: function complete(record, afterLoc) { if (\"throw\" === record.type) throw record.arg; return \"break\" === record.type || \"continue\" === record.type ? this.next = record.arg : \"return\" === record.type ? (this.rval = this.arg = record.arg, this.method = \"return\", this.next = \"end\") : \"normal\" === record.type && afterLoc && (this.next = afterLoc), ContinueSentinel; }, finish: function finish(finallyLoc) { for (var i = this.tryEntries.length - 1; i >= 0; --i) { var entry = this.tryEntries[i]; if (entry.finallyLoc === finallyLoc) return this.complete(entry.completion, entry.afterLoc), resetTryEntry(entry), ContinueSentinel; } }, catch: function _catch(tryLoc) { for (var i = this.tryEntries.length - 1; i >= 0; --i) { var entry = this.tryEntries[i]; if (entry.tryLoc === tryLoc) { var record = entry.completion; if (\"throw\" === record.type) { var thrown = record.arg; resetTryEntry(entry); } return thrown; } } throw new Error(\"illegal catch attempt\"); }, delegateYield: function delegateYield(iterable, resultName, nextLoc) { return this.delegate = { iterator: values(iterable), resultName: resultName, nextLoc: nextLoc }, \"next\" === this.method && (this.arg = undefined), ContinueSentinel; } }, exports; }\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { dispose as _dispose, io, Tensor, util } from '@tensorflow/tfjs-core';\nimport { OperationMapper } from '../operations/operation_mapper';\nimport { GraphExecutor } from './graph_executor';\nimport { ResourceManager } from './resource_manager';\nexport var TFHUB_SEARCH_PARAM = '?tfjs-format=file';\nexport var DEFAULT_MODEL_NAME = 'model.json';\n/**\n * A `tf.GraphModel` is a directed, acyclic graph built from a\n * SavedModel GraphDef and allows inference execution.\n *\n * A `tf.GraphModel` can only be created by loading from a model converted from\n * a [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model) using\n * the command line converter tool and loaded via `tf.loadGraphModel`.\n *\n * @doc {heading: 'Models', subheading: 'Classes'}\n */\nexport var GraphModel = /*#__PURE__*/function () {\n  /**\n   * @param modelUrl url for the model, or an `io.IOHandler`.\n   * @param weightManifestUrl url for the weight file generated by\n   * scripts/convert.py script.\n   * @param requestOption options for Request, which allows to send credentials\n   * and custom headers.\n   * @param onProgress Optional, progress callback function, fired periodically\n   * before the load is completed.\n   */\n  function GraphModel(modelUrl) {\n    var loadOptions = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    var tfio = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : io;\n    _classCallCheck(this, GraphModel);\n    this.modelUrl = modelUrl;\n    this.loadOptions = loadOptions;\n    this.version = 'n/a';\n    this.io = tfio;\n    if (loadOptions == null) {\n      this.loadOptions = {};\n    }\n    this.resourceManager = new ResourceManager();\n  }\n  // Returns the version information for the tensorflow model GraphDef.\n  _createClass(GraphModel, [{\n    key: \"findIOHandler\",\n    value: function findIOHandler() {\n      var path = this.modelUrl;\n      if (path.load != null) {\n        // Path is an IO Handler.\n        this.handler = path;\n      } else if (this.loadOptions.requestInit != null) {\n        this.handler = this.io.browserHTTPRequest(path, this.loadOptions);\n      } else {\n        var handlers = this.io.getLoadHandlers(path, this.loadOptions);\n        if (handlers.length === 0) {\n          // For backward compatibility: if no load handler can be found,\n          // assume it is a relative http path.\n          handlers.push(this.io.browserHTTPRequest(path, this.loadOptions));\n        } else if (handlers.length > 1) {\n          throw new Error(\"Found more than one (\".concat(handlers.length, \") load handlers for \") + \"URL '\".concat([path], \"'\"));\n        }\n        this.handler = handlers[0];\n      }\n    }\n    /**\n     * Loads the model and weight files, construct the in memory weight map and\n     * compile the inference graph.\n     */\n  }, {\n    key: \"load\",\n    value: function load() {\n      var _this = this;\n      this.findIOHandler();\n      if (this.handler.load == null) {\n        throw new Error('Cannot proceed with model loading because the IOHandler provided ' + 'does not have the `load` method implemented.');\n      }\n      var loadResult = this.handler.load();\n      if (util.isPromise(loadResult)) {\n        return loadResult.then(function (artifacts) {\n          return _this.loadSync(artifacts);\n        });\n      }\n      return this.loadSync(loadResult);\n    }\n    /**\n     * Synchronously construct the in memory weight map and\n     * compile the inference graph.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n     */\n  }, {\n    key: \"loadSync\",\n    value: function loadSync(artifacts) {\n      this.artifacts = artifacts;\n      var graph = this.artifacts.modelTopology;\n      var signature = this.artifacts.signature;\n      if (this.artifacts.userDefinedMetadata != null) {\n        var metadata = this.artifacts.userDefinedMetadata;\n        if (metadata.signature != null) {\n          signature = metadata.signature;\n        }\n        if (metadata.structuredOutputKeys != null) {\n          this.structuredOutputKeys = metadata.structuredOutputKeys;\n        }\n      }\n      this.signature = signature;\n      this.version = \"\".concat(graph.versions.producer, \".\").concat(graph.versions.minConsumer);\n      var weightMap = this.io.decodeWeights(this.artifacts.weightData, this.artifacts.weightSpecs);\n      this.executor = new GraphExecutor(OperationMapper.Instance.transformGraph(graph, this.signature));\n      this.executor.weightMap = this.convertTensorMapToTensorsMap(weightMap);\n      // Attach a model-level resourceManager to each executor to share resources,\n      // such as `HashTable`.\n      this.executor.resourceManager = this.resourceManager;\n      if (artifacts.modelInitializer != null && artifacts.modelInitializer.node != null) {\n        var initializer = OperationMapper.Instance.transformGraph(artifacts.modelInitializer);\n        this.initializer = new GraphExecutor(initializer);\n        this.initializer.weightMap = this.executor.weightMap;\n        // Attach a model-level resourceManager to the initializer, the\n        // hashTables created from when executing the initializer will be stored\n        // in the resourceManager.\n        this.initializer.resourceManager = this.resourceManager;\n        this.initializerSignature = artifacts.initializerSignature;\n      }\n      return true;\n    }\n    /**\n     * Save the configuration and/or weights of the GraphModel.\n     *\n     * An `IOHandler` is an object that has a `save` method of the proper\n     * signature defined. The `save` method manages the storing or\n     * transmission of serialized data (\"artifacts\") that represent the\n     * model's topology and weights onto or via a specific medium, such as\n     * file downloads, local storage, IndexedDB in the web browser and HTTP\n     * requests to a server. TensorFlow.js provides `IOHandler`\n     * implementations for a number of frequently used saving mediums, such as\n     * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`\n     * for more details.\n     *\n     * This method also allows you to refer to certain types of `IOHandler`s\n     * as URL-like string shortcuts, such as 'localstorage://' and\n     * 'indexeddb://'.\n     *\n     * Example 1: Save `model`'s topology and weights to browser [local\n     * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);\n     * then load it back.\n     *\n     * ```js\n     * const modelUrl =\n     *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\n     * const model = await tf.loadGraphModel(modelUrl);\n     * const zeros = tf.zeros([1, 224, 224, 3]);\n     * model.predict(zeros).print();\n     *\n     * const saveResults = await model.save('localstorage://my-model-1');\n     *\n     * const loadedModel = await tf.loadGraphModel('localstorage://my-model-1');\n     * console.log('Prediction from loaded model:');\n     * model.predict(zeros).print();\n     * ```\n     *\n     * @param handlerOrURL An instance of `IOHandler` or a URL-like,\n     * scheme-based string shortcut for `IOHandler`.\n     * @param config Options for saving the model.\n     * @returns A `Promise` of `SaveResult`, which summarizes the result of\n     * the saving, such as byte sizes of the saved artifacts for the model's\n     *   topology and weight values.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n     */\n  }, {\n    key: \"save\",\n    value: function () {\n      var _save = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee(handlerOrURL, config) {\n        var handlers;\n        return _regeneratorRuntime().wrap(function _callee$(_context) {\n          while (1) {\n            switch (_context.prev = _context.next) {\n              case 0:\n                if (!(typeof handlerOrURL === 'string')) {\n                  _context.next = 9;\n                  break;\n                }\n                handlers = this.io.getSaveHandlers(handlerOrURL);\n                if (!(handlers.length === 0)) {\n                  _context.next = 6;\n                  break;\n                }\n                throw new Error(\"Cannot find any save handlers for URL '\".concat(handlerOrURL, \"'\"));\n              case 6:\n                if (!(handlers.length > 1)) {\n                  _context.next = 8;\n                  break;\n                }\n                throw new Error(\"Found more than one (\".concat(handlers.length, \") save handlers for \") + \"URL '\".concat(handlerOrURL, \"'\"));\n              case 8:\n                handlerOrURL = handlers[0];\n              case 9:\n                if (!(handlerOrURL.save == null)) {\n                  _context.next = 11;\n                  break;\n                }\n                throw new Error('GraphModel.save() cannot proceed because the IOHandler ' + 'provided does not have the `save` attribute defined.');\n              case 11:\n                return _context.abrupt(\"return\", handlerOrURL.save(this.artifacts));\n              case 12:\n              case \"end\":\n                return _context.stop();\n            }\n          }\n        }, _callee, this);\n      }));\n      function save(_x, _x2) {\n        return _save.apply(this, arguments);\n      }\n      return save;\n    }()\n  }, {\n    key: \"addStructuredOutputNames\",\n    value: function addStructuredOutputNames(outputTensors) {\n      var _this2 = this;\n      if (this.structuredOutputKeys) {\n        var outputTensorsArray = outputTensors instanceof Tensor ? [outputTensors] : outputTensors;\n        var outputTensorMap = {};\n        outputTensorsArray.forEach(function (outputTensor, i) {\n          return outputTensorMap[_this2.structuredOutputKeys[i]] = outputTensor;\n        });\n        return outputTensorMap;\n      }\n      return outputTensors;\n    }\n    /**\n     * Execute the inference for the input tensors.\n     *\n     * @param input The input tensors, when there is single input for the model,\n     * inputs param should be a `tf.Tensor`. For models with mutliple inputs,\n     * inputs params should be in either `tf.Tensor`[] if the input order is\n     * fixed, or otherwise NamedTensorMap format.\n     *\n     * For model with multiple inputs, we recommend you use NamedTensorMap as the\n     * input type, if you use `tf.Tensor`[], the order of the array needs to\n     * follow the\n     * order of inputNodes array. @see {@link GraphModel.inputNodes}\n     *\n     * You can also feed any intermediate nodes using the NamedTensorMap as the\n     * input type. For example, given the graph\n     *    InputNode => Intermediate => OutputNode,\n     * you can execute the subgraph Intermediate => OutputNode by calling\n     *    model.execute('IntermediateNode' : tf.tensor(...));\n     *\n     * This is useful for models that uses tf.dynamic_rnn, where the intermediate\n     * state needs to be fed manually.\n     *\n     * For batch inference execution, the tensors for each input need to be\n     * concatenated together. For example with mobilenet, the required input shape\n     * is [1, 244, 244, 3], which represents the [batch, height, width, channel].\n     * If we are provide a batched data of 100 images, the input tensor should be\n     * in the shape of [100, 244, 244, 3].\n     *\n     * @param config Prediction configuration for specifying the batch size.\n     * Currently the batch size option is ignored for graph model.\n     *\n     * @returns Inference result tensors. If the model is converted and it\n     * originally had structured_outputs in tensorflow, then a NamedTensorMap\n     * will be returned matching the structured_outputs. If no structured_outputs\n     * are present, the output will be single `tf.Tensor` if the model has single\n     * output node, otherwise Tensor[].\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n  }, {\n    key: \"predict\",\n    value: function predict(inputs, config) {\n      var outputTensors = this.execute(inputs, this.outputNodes);\n      return this.addStructuredOutputNames(outputTensors);\n    }\n    /**\n     * Execute the inference for the input tensors in async fashion, use this\n     * method when your model contains control flow ops.\n     *\n     * @param input The input tensors, when there is single input for the model,\n     * inputs param should be a `tf.Tensor`. For models with mutliple inputs,\n     * inputs params should be in either `tf.Tensor`[] if the input order is\n     * fixed, or otherwise NamedTensorMap format.\n     *\n     * For model with multiple inputs, we recommend you use NamedTensorMap as the\n     * input type, if you use `tf.Tensor`[], the order of the array needs to\n     * follow the\n     * order of inputNodes array. @see {@link GraphModel.inputNodes}\n     *\n     * You can also feed any intermediate nodes using the NamedTensorMap as the\n     * input type. For example, given the graph\n     *    InputNode => Intermediate => OutputNode,\n     * you can execute the subgraph Intermediate => OutputNode by calling\n     *    model.execute('IntermediateNode' : tf.tensor(...));\n     *\n     * This is useful for models that uses tf.dynamic_rnn, where the intermediate\n     * state needs to be fed manually.\n     *\n     * For batch inference execution, the tensors for each input need to be\n     * concatenated together. For example with mobilenet, the required input shape\n     * is [1, 244, 244, 3], which represents the [batch, height, width, channel].\n     * If we are provide a batched data of 100 images, the input tensor should be\n     * in the shape of [100, 244, 244, 3].\n     *\n     * @param config Prediction configuration for specifying the batch size.\n     * Currently the batch size option is ignored for graph model.\n     *\n     * @returns A Promise of inference result tensors. If the model is converted\n     * and it originally had structured_outputs in tensorflow, then a\n     * NamedTensorMap will be returned matching the structured_outputs. If no\n     * structured_outputs are present, the output will be single `tf.Tensor` if\n     * the model has single output node, otherwise Tensor[].\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n  }, {\n    key: \"predictAsync\",\n    value: function () {\n      var _predictAsync = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee2(inputs, config) {\n        var outputTensors;\n        return _regeneratorRuntime().wrap(function _callee2$(_context2) {\n          while (1) {\n            switch (_context2.prev = _context2.next) {\n              case 0:\n                _context2.next = 2;\n                return this.executeAsync(inputs, this.outputNodes);\n              case 2:\n                outputTensors = _context2.sent;\n                return _context2.abrupt(\"return\", this.addStructuredOutputNames(outputTensors));\n              case 4:\n              case \"end\":\n                return _context2.stop();\n            }\n          }\n        }, _callee2, this);\n      }));\n      function predictAsync(_x3, _x4) {\n        return _predictAsync.apply(this, arguments);\n      }\n      return predictAsync;\n    }()\n  }, {\n    key: \"normalizeInputs\",\n    value: function normalizeInputs(inputs) {\n      var _this3 = this;\n      var _a;\n      if (!(inputs instanceof Tensor) && !Array.isArray(inputs)) {\n        // The input is already a NamedTensorMap.\n        var signatureInputs = (_a = this.signature) === null || _a === void 0 ? void 0 : _a.inputs;\n        if (signatureInputs != null) {\n          for (var input in signatureInputs) {\n            var tensor = signatureInputs[input];\n            if (tensor.resourceId != null) {\n              inputs[input] = this.resourceIdToCapturedInput[tensor.resourceId];\n            }\n          }\n        }\n        return inputs;\n      }\n      inputs = Array.isArray(inputs) ? inputs : [inputs];\n      var numCapturedInputs = Object.keys(this.resourceIdToCapturedInput).length;\n      if (inputs.length + numCapturedInputs !== this.inputNodes.length) {\n        throw new Error(\"Input tensor count mismatch, the graph model has \".concat(this.inputNodes.length - numCapturedInputs, \" non-resource placeholders, while there are \").concat(inputs.length, \" input tensors provided.\"));\n      }\n      var inputIndex = 0;\n      return this.inputNodes.reduce(function (map, inputName) {\n        var _a, _b, _c;\n        var resourceId = (_c = (_b = (_a = _this3.signature) === null || _a === void 0 ? void 0 : _a.inputs) === null || _b === void 0 ? void 0 : _b[inputName]) === null || _c === void 0 ? void 0 : _c.resourceId;\n        if (resourceId != null) {\n          map[inputName] = _this3.resourceIdToCapturedInput[resourceId];\n        } else {\n          map[inputName] = inputs[inputIndex++];\n        }\n        return map;\n      }, {});\n    }\n  }, {\n    key: \"normalizeOutputs\",\n    value: function normalizeOutputs(outputs) {\n      outputs = outputs || this.outputNodes;\n      return !Array.isArray(outputs) ? [outputs] : outputs;\n    }\n  }, {\n    key: \"executeInitializerGraph\",\n    value: function executeInitializerGraph() {\n      if (this.initializer == null) {\n        return [];\n      }\n      if (this.initializerSignature == null) {\n        return this.initializer.execute({}, []);\n      } else {\n        return this.initializer.execute({}, Object.keys(this.initializerSignature.outputs));\n      }\n    }\n  }, {\n    key: \"executeInitializerGraphAsync\",\n    value: function () {\n      var _executeInitializerGraphAsync = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee3() {\n        return _regeneratorRuntime().wrap(function _callee3$(_context3) {\n          while (1) {\n            switch (_context3.prev = _context3.next) {\n              case 0:\n                if (!(this.initializer == null)) {\n                  _context3.next = 2;\n                  break;\n                }\n                return _context3.abrupt(\"return\", []);\n              case 2:\n                if (!(this.initializerSignature == null)) {\n                  _context3.next = 6;\n                  break;\n                }\n                return _context3.abrupt(\"return\", this.initializer.executeAsync({}, []));\n              case 6:\n                return _context3.abrupt(\"return\", this.initializer.executeAsync({}, Object.keys(this.initializerSignature.outputs)));\n              case 7:\n              case \"end\":\n                return _context3.stop();\n            }\n          }\n        }, _callee3, this);\n      }));\n      function executeInitializerGraphAsync() {\n        return _executeInitializerGraphAsync.apply(this, arguments);\n      }\n      return executeInitializerGraphAsync;\n    }()\n  }, {\n    key: \"setResourceIdToCapturedInput\",\n    value: function setResourceIdToCapturedInput(outputs) {\n      this.resourceIdToCapturedInput = {};\n      if (this.initializerSignature) {\n        var signatureOutputs = this.initializerSignature.outputs;\n        var outputNames = Object.keys(signatureOutputs);\n        for (var i = 0; i < outputNames.length; i++) {\n          var outputName = outputNames[i];\n          var tensorInfo = signatureOutputs[outputName];\n          this.resourceIdToCapturedInput[tensorInfo.resourceId] = outputs[i];\n        }\n      }\n    }\n    /**\n     * Executes inference for the model for given input tensors.\n     * @param inputs tensor, tensor array or tensor map of the inputs for the\n     * model, keyed by the input node names.\n     * @param outputs output node name from the TensorFlow model, if no\n     * outputs are specified, the default outputs of the model would be used.\n     * You can inspect intermediate nodes of the model by adding them to the\n     * outputs array.\n     *\n     * @returns A single tensor if provided with a single output or no outputs\n     * are provided and there is only one default output, otherwise return a\n     * tensor array. The order of the tensor array is the same as the outputs\n     * if provided, otherwise the order of outputNodes attribute of the model.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n  }, {\n    key: \"execute\",\n    value: function execute(inputs, outputs) {\n      if (this.resourceIdToCapturedInput == null) {\n        this.setResourceIdToCapturedInput(this.executeInitializerGraph());\n      }\n      inputs = this.normalizeInputs(inputs);\n      outputs = this.normalizeOutputs(outputs);\n      var result = this.executor.execute(inputs, outputs);\n      return result.length > 1 ? result : result[0];\n    }\n    /**\n     * Executes inference for the model for given input tensors in async\n     * fashion, use this method when your model contains control flow ops.\n     * @param inputs tensor, tensor array or tensor map of the inputs for the\n     * model, keyed by the input node names.\n     * @param outputs output node name from the TensorFlow model, if no outputs\n     * are specified, the default outputs of the model would be used. You can\n     * inspect intermediate nodes of the model by adding them to the outputs\n     * array.\n     *\n     * @returns A Promise of single tensor if provided with a single output or\n     * no outputs are provided and there is only one default output, otherwise\n     * return a tensor map.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n  }, {\n    key: \"executeAsync\",\n    value: function () {\n      var _executeAsync = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee4(inputs, outputs) {\n        var result;\n        return _regeneratorRuntime().wrap(function _callee4$(_context4) {\n          while (1) {\n            switch (_context4.prev = _context4.next) {\n              case 0:\n                if (!(this.resourceIdToCapturedInput == null)) {\n                  _context4.next = 6;\n                  break;\n                }\n                _context4.t0 = this;\n                _context4.next = 4;\n                return this.executeInitializerGraphAsync();\n              case 4:\n                _context4.t1 = _context4.sent;\n                _context4.t0.setResourceIdToCapturedInput.call(_context4.t0, _context4.t1);\n              case 6:\n                inputs = this.normalizeInputs(inputs);\n                outputs = this.normalizeOutputs(outputs);\n                _context4.next = 10;\n                return this.executor.executeAsync(inputs, outputs);\n              case 10:\n                result = _context4.sent;\n                return _context4.abrupt(\"return\", result.length > 1 ? result : result[0]);\n              case 12:\n              case \"end\":\n                return _context4.stop();\n            }\n          }\n        }, _callee4, this);\n      }));\n      function executeAsync(_x5, _x6) {\n        return _executeAsync.apply(this, arguments);\n      }\n      return executeAsync;\n    }()\n    /**\n     * Get intermediate tensors for model debugging mode (flag\n     * KEEP_INTERMEDIATE_TENSORS is true).\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n  }, {\n    key: \"getIntermediateTensors\",\n    value: function getIntermediateTensors() {\n      return this.executor.getIntermediateTensors();\n    }\n    /**\n     * Dispose intermediate tensors for model debugging mode (flag\n     * KEEP_INTERMEDIATE_TENSORS is true).\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n  }, {\n    key: \"disposeIntermediateTensors\",\n    value: function disposeIntermediateTensors() {\n      this.executor.disposeIntermediateTensors();\n    }\n  }, {\n    key: \"convertTensorMapToTensorsMap\",\n    value: function convertTensorMapToTensorsMap(map) {\n      return Object.keys(map).reduce(function (newMap, key) {\n        newMap[key] = [map[key]];\n        return newMap;\n      }, {});\n    }\n    /**\n     * Releases the memory used by the weight tensors and resourceManager.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      this.executor.dispose();\n      if (this.initializer) {\n        this.initializer.dispose();\n        if (this.resourceIdToCapturedInput) {\n          _dispose(this.resourceIdToCapturedInput);\n        }\n      }\n      this.resourceManager.dispose();\n    }\n  }, {\n    key: \"modelVersion\",\n    get: function get() {\n      return this.version;\n    }\n  }, {\n    key: \"inputNodes\",\n    get: function get() {\n      return this.executor.inputNodes;\n    }\n  }, {\n    key: \"outputNodes\",\n    get: function get() {\n      return this.executor.outputNodes;\n    }\n  }, {\n    key: \"inputs\",\n    get: function get() {\n      return this.executor.inputs;\n    }\n  }, {\n    key: \"outputs\",\n    get: function get() {\n      return this.executor.outputs;\n    }\n  }, {\n    key: \"weights\",\n    get: function get() {\n      return this.executor.weightMap;\n    }\n  }, {\n    key: \"metadata\",\n    get: function get() {\n      return this.artifacts.userDefinedMetadata;\n    }\n  }, {\n    key: \"modelSignature\",\n    get: function get() {\n      return this.signature;\n    }\n  }, {\n    key: \"modelStructuredOutputKeys\",\n    get: function get() {\n      return this.structuredOutputKeys;\n    }\n  }]);\n  return GraphModel;\n}();\n/**\n * Load a graph model given a URL to the model definition.\n *\n * Example of loading MobileNetV2 from a URL and making a prediction with a\n * zeros input:\n *\n * ```js\n * const modelUrl =\n *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\n * const model = await tf.loadGraphModel(modelUrl);\n * const zeros = tf.zeros([1, 224, 224, 3]);\n * model.predict(zeros).print();\n * ```\n *\n * Example of loading MobileNetV2 from a TF Hub URL and making a prediction\n * with a zeros input:\n *\n * ```js\n * const modelUrl =\n *    'https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/2';\n * const model = await tf.loadGraphModel(modelUrl, {fromTFHub: true});\n * const zeros = tf.zeros([1, 224, 224, 3]);\n * model.predict(zeros).print();\n * ```\n * @param modelUrl The url or an `io.IOHandler` that loads the model.\n * @param options Options for the HTTP request, which allows to send\n *     credentials\n *    and custom headers.\n *\n * @doc {heading: 'Models', subheading: 'Loading'}\n */\nexport function loadGraphModel(_x7) {\n  return _loadGraphModel.apply(this, arguments);\n}\n/**\n * Load a graph model given a synchronous IO handler with a 'load' method.\n *\n * @param modelSource The `io.IOHandlerSync` that loads the model, or the\n *     `io.ModelArtifacts` that encode the model, or a tuple of\n *     `[io.ModelJSON, ArrayBuffer]` of which the first element encodes the\n *      model and the second contains the weights.\n *\n * @doc {heading: 'Models', subheading: 'Loading'}\n */\nfunction _loadGraphModel() {\n  _loadGraphModel = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee5(modelUrl) {\n    var options,\n      tfio,\n      model,\n      _args5 = arguments;\n    return _regeneratorRuntime().wrap(function _callee5$(_context5) {\n      while (1) {\n        switch (_context5.prev = _context5.next) {\n          case 0:\n            options = _args5.length > 1 && _args5[1] !== undefined ? _args5[1] : {};\n            tfio = _args5.length > 2 && _args5[2] !== undefined ? _args5[2] : io;\n            if (!(modelUrl == null)) {\n              _context5.next = 4;\n              break;\n            }\n            throw new Error('modelUrl in loadGraphModel() cannot be null. Please provide a url ' + 'or an IOHandler that loads the model');\n          case 4:\n            if (options == null) {\n              options = {};\n            }\n            if (options.fromTFHub && typeof modelUrl === 'string') {\n              modelUrl = getTFHubUrl(modelUrl);\n            }\n            model = new GraphModel(modelUrl, options, tfio);\n            _context5.next = 9;\n            return model.load();\n          case 9:\n            return _context5.abrupt(\"return\", model);\n          case 10:\n          case \"end\":\n            return _context5.stop();\n        }\n      }\n    }, _callee5);\n  }));\n  return _loadGraphModel.apply(this, arguments);\n}\nexport function loadGraphModelSync(modelSource) {\n  if (modelSource == null) {\n    throw new Error('modelUrl in loadGraphModelSync() cannot be null. Please provide ' + 'model artifacts or an IOHandler that loads the model');\n  }\n  var ioHandler;\n  if (modelSource instanceof Array) {\n    var _modelSource = _slicedToArray(modelSource, 2),\n      modelJSON = _modelSource[0],\n      weights = _modelSource[1];\n    if (!modelJSON) {\n      throw new Error('modelJSON must be the first element of the array');\n    }\n    if (!weights || !(weights instanceof ArrayBuffer)) {\n      throw new Error('An ArrayBuffer of weights must be the second element of' + ' the array');\n    }\n    if (!('modelTopology' in modelJSON)) {\n      throw new Error('Model JSON is missing \\'modelTopology\\'');\n    }\n    if (!('weightsManifest' in modelJSON)) {\n      throw new Error('Model JSON is missing \\'weightsManifest\\'');\n    }\n    var weightSpecs = io.getWeightSpecs(modelJSON.weightsManifest);\n    var modelArtifacts = io.getModelArtifactsForJSONSync(modelJSON, weightSpecs, weights);\n    ioHandler = io.fromMemorySync(modelArtifacts);\n  } else if ('load' in modelSource) {\n    // Then modelSource is already an IOHandlerSync.\n    ioHandler = modelSource;\n  } else if ('modelTopology' in modelSource && 'weightSpecs' in modelSource && 'weightData' in modelSource) {\n    // modelSource is of type ModelArtifacts.\n    ioHandler = io.fromMemorySync(modelSource);\n  } else {\n    throw new Error('Unknown model format');\n  }\n  var model = new GraphModel(ioHandler);\n  model.load();\n  return model;\n}\nfunction getTFHubUrl(modelUrl) {\n  if (!modelUrl.endsWith('/')) {\n    modelUrl = modelUrl + '/';\n  }\n  return \"\".concat(modelUrl).concat(DEFAULT_MODEL_NAME).concat(TFHUB_SEARCH_PARAM);\n}","map":null,"metadata":{},"sourceType":"module"}