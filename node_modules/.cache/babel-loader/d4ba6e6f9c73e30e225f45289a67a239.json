{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { backend_util, env, FusedConv2D, util } from '@tensorflow/tfjs-core';\nimport { Conv2DProgram } from '../conv_gpu';\nimport { Conv2DPackedProgram } from '../conv_packed_gpu';\nimport { mapActivationToShaderProgram } from '../kernel_utils/kernel_funcs_utils';\nimport { conv2dByMatMul, conv2dWithIm2Row } from './Conv2D_impl';\nimport { reshape } from './Reshape';\nexport function fusedConv2d(args) {\n  var inputs = args.inputs,\n    backend = args.backend,\n    attrs = args.attrs;\n  var x = inputs.x,\n    filter = inputs.filter,\n    bias = inputs.bias,\n    preluActivationWeights = inputs.preluActivationWeights;\n  var strides = attrs.strides,\n    pad = attrs.pad,\n    dataFormat = attrs.dataFormat,\n    dilations = attrs.dilations,\n    dimRoundingMode = attrs.dimRoundingMode,\n    activation = attrs.activation,\n    leakyreluAlpha = attrs.leakyreluAlpha;\n  var $dataFormat = backend_util.convertConv2DDataFormat(dataFormat);\n  var convInfo = backend_util.computeConv2DInfo(x.shape, filter.shape, strides, dilations, pad, dimRoundingMode, false /* depthwise */, $dataFormat);\n  var out;\n  var intermediates = [];\n  var hasBias = bias != null;\n  var hasPreluActivationWeights = preluActivationWeights != null;\n  var hasLeakyreluAlpha = activation === 'leakyrelu';\n  var prepareInputs = function prepareInputs() {\n    var inputs = [x, filter];\n    // If the input is a 1-D tensor, align it with the channels.\n    //\n    // For fusedConv2d, the inputs (x, W, bias, preluActivationWeights) are\n    // supposed to be aligned with the dataFormat. The 4-D tensor inputs or\n    // scalar inputs are originally aligned, but the 1-D tensor inputs are\n    // supposed to be aligned with the channels (only bias and PReLU activation\n    // weights could be a 1-D tensor).\n    var alignInputWithDataFormat = function alignInputWithDataFormat(input, dataFormat) {\n      if (dataFormat === 'NCHW' && input.shape.length === 1 && input.shape[0] !== 1) {\n        var alignedInput = reshape({\n          inputs: {\n            x: input\n          },\n          backend: backend,\n          attrs: {\n            shape: [input.shape[0], 1, 1]\n          }\n        });\n        intermediates.push(alignedInput);\n        return alignedInput;\n      }\n      return input;\n    };\n    if (hasBias) {\n      inputs.push(alignInputWithDataFormat(bias, dataFormat));\n    }\n    if (hasPreluActivationWeights) {\n      inputs.push(alignInputWithDataFormat(preluActivationWeights, dataFormat));\n    }\n    if (hasLeakyreluAlpha) {\n      var $leakyreluAlpha = backend.makeTensorInfo([], 'float32', util.createScalarValue(leakyreluAlpha, 'float32'));\n      inputs.push($leakyreluAlpha);\n      intermediates.push($leakyreluAlpha);\n    }\n    return inputs;\n  };\n  if (convInfo.filterHeight === 1 && convInfo.filterWidth === 1 && convInfo.dilationHeight === 1 && convInfo.dilationWidth === 1 && convInfo.strideHeight === 1 && convInfo.strideWidth === 1 && (convInfo.padInfo.type === 'SAME' || convInfo.padInfo.type === 'VALID')) {\n    out = conv2dByMatMul({\n      x: x,\n      filter: filter,\n      convInfo: convInfo,\n      backend: backend,\n      bias: bias,\n      activation: activation,\n      preluActivationWeights: preluActivationWeights,\n      leakyreluAlpha: leakyreluAlpha\n    });\n  } else if (convInfo.strideWidth <= 2 && $dataFormat === 'channelsLast' && env().getBool('WEBGL_EXP_CONV')) {\n    var fusedActivation = activation ? mapActivationToShaderProgram(activation, true) : null;\n    var program = new Conv2DPackedProgram(convInfo, hasBias, fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n    var customValues = [[convInfo.padInfo.top, convInfo.padInfo.left], [convInfo.strideHeight, convInfo.strideWidth], [convInfo.dilationHeight, convInfo.dilationWidth], [convInfo.inHeight, convInfo.inWidth]];\n    var _inputs = prepareInputs();\n    out = backend.runWebGLProgram(program, _inputs, 'float32', customValues);\n  } else if (env().getBool('WEBGL_CONV_IM2COL')) {\n    out = conv2dWithIm2Row({\n      x: x,\n      filter: filter,\n      convInfo: convInfo,\n      backend: backend,\n      bias: bias,\n      activation: activation,\n      preluActivationWeights: preluActivationWeights,\n      leakyreluAlpha: leakyreluAlpha\n    });\n  } else {\n    var _fusedActivation = activation ? mapActivationToShaderProgram(activation, false) : null;\n    var _program = new Conv2DProgram(convInfo, hasBias, _fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n    var _inputs2 = prepareInputs();\n    out = backend.runWebGLProgram(_program, _inputs2, 'float32');\n  }\n  var outReshaped = reshape({\n    inputs: {\n      x: out\n    },\n    backend: backend,\n    attrs: {\n      shape: convInfo.outShape\n    }\n  });\n  intermediates.push(out);\n  intermediates.forEach(function (t) {\n    return backend.disposeIntermediateTensorInfo(t);\n  });\n  return outReshaped;\n}\nexport var fusedConv2DConfig = {\n  kernelName: FusedConv2D,\n  backendName: 'webgl',\n  kernelFunc: fusedConv2d\n};","map":null,"metadata":{},"sourceType":"module"}