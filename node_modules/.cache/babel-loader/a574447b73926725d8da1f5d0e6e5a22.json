{"ast":null,"code":"import _slicedToArray from \"C:\\\\Users\\\\Rohith\\\\Pictures\\\\frontend\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/helpers/esm/slicedToArray\";\n/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { FusedConv2D } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { conv2d as unfusedConv2d } from '../conv2d';\nimport { conv2DBackpropFilter } from '../conv2d_backprop_filter';\nimport { conv2DBackpropInput } from '../conv2d_backprop_input';\nimport * as conv_util from '../conv_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes a 2D convolution over the input x, optionally fused with adding a\n * bias and applying an activation.\n *\n * ```js\n * const inputDepth = 2;\n * const inShape = [2, 2, 2, inputDepth];\n * const outputDepth = 2;\n * const fSize = 1;\n * const pad = 0;\n * const strides = 1;\n *\n * const x = tf.tensor4d( [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n * 16], inShape);\n * const w = tf.tensor4d([-1, 1, -2, 0.5], [fSize, fSize, inputDepth,\n * outputDepth]);\n *\n * tf.fused.conv2d({ x, filter: w, strides, pad, dataFormat: 'NHWC',\n * dilations: [1, 1], bias: tf.scalar(5), activation: 'relu' }).print();\n * ```\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter, rank 4, of shape\n *     `[filterHeight, filterWidth, inDepth, outDepth]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid` output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](\n *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)\n * @param dataFormat An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`) to be\n *     applied\n *      after biasAdd.\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\nfunction fusedConv2d_(_ref) {\n  var x = _ref.x,\n    filter = _ref.filter,\n    strides = _ref.strides,\n    pad = _ref.pad,\n    _ref$dataFormat = _ref.dataFormat,\n    dataFormat = _ref$dataFormat === void 0 ? 'NHWC' : _ref$dataFormat,\n    _ref$dilations = _ref.dilations,\n    dilations = _ref$dilations === void 0 ? [1, 1] : _ref$dilations,\n    dimRoundingMode = _ref.dimRoundingMode,\n    bias = _ref.bias,\n    _ref$activation = _ref.activation,\n    activation = _ref$activation === void 0 ? 'linear' : _ref$activation,\n    preluActivationWeights = _ref.preluActivationWeights,\n    leakyreluAlpha = _ref.leakyreluAlpha;\n  activation = activation || 'linear';\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    // TODO: Transpose bias and preluActivationWeights properly for NCHW\n    // format before computation.\n    util.assert(dataFormat === 'NHWC', function () {\n      return \"Error in fused conv2d: got dataFormat of \".concat(dataFormat, \" but \") + \"only NHWC is currently supported for the case of gradient depth \" + \"is 0 and the activation is not linear.\";\n    });\n    var result = unfusedConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n    if (bias != null) {\n      result = add(result, bias);\n    }\n    return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n  }\n  var $x = convertToTensor(x, 'x', 'conv2d', 'float32');\n  var $filter = convertToTensor(filter, 'filter', 'conv2d', 'float32');\n  var x4D = $x;\n  var reshapedTo4D = false;\n  if ($x.rank === 3) {\n    reshapedTo4D = true;\n    x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n  }\n  util.assert(x4D.rank === 4, function () {\n    return \"Error in fused conv2d: input must be rank 4, but got rank \" + \"\".concat(x4D.rank, \".\");\n  });\n  util.assert($filter.rank === 4, function () {\n    return \"Error in fused conv2d: filter must be rank 4, but got rank \" + \"\".concat($filter.rank, \".\");\n  });\n  conv_util.checkPadOnDimRoundingMode('fused conv2d', pad, dimRoundingMode);\n  var inputChannels = dataFormat === 'NHWC' ? x4D.shape[3] : x4D.shape[1];\n  util.assert($filter.shape[2] === inputChannels, function () {\n    return \"Error in conv2d: depth of input (\".concat(inputChannels, \") must match \") + \"input depth for filter \".concat($filter.shape[2], \".\");\n  });\n  util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), function () {\n    return 'Error in conv2D: Either strides or dilations must be 1. ' + \"Got strides \".concat(strides, \" and dilations '\").concat(dilations, \"'\");\n  });\n  var convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode);\n  var $bias;\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n    var _makeTypesMatch = makeTypesMatch($bias, $x);\n    var _makeTypesMatch2 = _slicedToArray(_makeTypesMatch, 1);\n    $bias = _makeTypesMatch2[0];\n    // According to TensorFlow, the bias is supposed be a 1-D tensor or a\n    // scalar.\n    //\n    // 3-D or 4-D bias is not disabled for NHWC format, because they are\n    // currently being used in some cases. For examplem in our code base,\n    // https://github.com/tensorflow/tfjs/blob/b53bd47e880367ae57493f0ea628abaf08db2d5d/tfjs-core/src/ops/fused/fused_conv2d_test.ts#L1972.\n    if (dataFormat === 'NHWC') {\n      broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n    } else {\n      util.assert($bias.shape.length <= 1, function () {\n        return \"Error in fused conv2d: only supports scalar or 1-D Tensor \" + \"bias for NCHW format but got the bias of \" + \"rank-\".concat($bias.shape.length, \".\");\n      });\n      util.assert($bias.shape.length === 0 || $bias.shape[0] === convInfo.outChannels || $bias.shape[0] === 1, function () {\n        return \"Error in fused conv2d: bias shape (\".concat($bias.shape, \") is not \") + \"compatible with the number of output channels \" + \"(\".concat(convInfo.outChannels, \")\");\n      });\n    }\n  }\n  var $preluActivationWeights;\n  if (preluActivationWeights != null) {\n    // PReLU's activation weights could be a scalar, a 1-D tensor or a 3-D\n    // tensor.\n    var alphaShape = preluActivationWeights.shape;\n    util.assert(alphaShape.length <= 1 || alphaShape.length === 3, function () {\n      return \"Error in fused conv2d: only supports scalar, 1-D Tensor or \" + \"3-D Tensor PReLU activation weights but got a tensor of \" + \"rank-\".concat(alphaShape.length, \".\");\n    });\n    if (alphaShape.length === 1) {\n      // Whether the data format is NCHW or NHWC, the 1-D PReLU activation\n      // weights tensor should be aligned with the output channels of conv2d\n      // result.\n      util.assert(alphaShape[0] === 1 || alphaShape[0] === convInfo.outChannels, function () {\n        return \"Error in fused conv2d: PReLU activation weights \" + \"(\".concat(alphaShape, \") is not compatible with the number of output \") + \"channels (\".concat(convInfo.outChannels, \").\");\n      });\n    } else if (alphaShape.length === 3) {\n      // Whether the data format is NCHW or NHWC, the PReLU activation weights\n      // tensor should has the compatible shape with the result of conv2d.\n      try {\n        broadcast_util.assertAndGetBroadcastShape(alphaShape, convInfo.outShape);\n      } catch (e) {\n        var errMsg = \"Error in fused conv2d: PReLU activation weights (\".concat(alphaShape, \") \") + \"is not compatible with the output shape of the conv2d \" + \"(\".concat(convInfo.outShape, \").\");\n        throw Error(errMsg);\n      }\n    }\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused conv2d');\n  }\n  var grad = function grad(dy, saved) {\n    util.assert(dataFormat === 'NHWC', function () {\n      return \"Error in gradient of fused conv2D: got dataFormat of \".concat(dataFormat, \" but only NHWC is currently supported.\");\n    });\n    var _saved = _slicedToArray(saved, 4),\n      $filter = _saved[0],\n      x4D = _saved[1],\n      y = _saved[2],\n      $bias = _saved[3];\n    var dyActivation = getFusedDyActivation(dy, y, activation);\n    util.assert(conv_util.tupleValuesAreOne(dilations), function () {\n      return 'Error in gradient of fused conv2D: ' + \"dilation rates greater than 1 \" + \"are not yet supported in gradients. Got dilations '\".concat(dilations, \"'\");\n    });\n    var xDer = conv2DBackpropInput(x4D.shape, dyActivation, $filter, strides, pad);\n    var filterDer = conv2DBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad);\n    var der = [xDer, filterDer];\n    if ($bias != null) {\n      var biasDer = getFusedBiasGradient($bias, dyActivation);\n      der.push(biasDer);\n    }\n    return der;\n  };\n  var inputs = {\n    x: x4D,\n    filter: $filter,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  var attrs = {\n    strides: strides,\n    pad: pad,\n    dataFormat: dataFormat,\n    dilations: dilations,\n    dimRoundingMode: dimRoundingMode,\n    activation: activation,\n    leakyreluAlpha: leakyreluAlpha\n  };\n  // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n  if (bias == null) {\n    var customOp = customGrad(function (x4D, filter, save) {\n      var res =\n      // tslint:disable-next-line: no-unnecessary-type-assertion\n      ENGINE.runKernel(FusedConv2D, inputs, attrs);\n      save([filter, x4D, res]);\n      if (reshapedTo4D) {\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOp(x4D, $filter);\n  } else {\n    var customOpWithBias = customGrad(function (x4D, filter, bias, save) {\n      var res = ENGINE.runKernel(FusedConv2D, inputs, attrs);\n      save([filter, x4D, res, bias]);\n      if (reshapedTo4D) {\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOpWithBias(x4D, $filter, $bias);\n  }\n}\nexport var conv2d = op({\n  fusedConv2d_: fusedConv2d_\n});","map":null,"metadata":{},"sourceType":"module"}